import numpy as np


#transition_data is a NumPy tensor of shape (N, T, 5) with the last last dimension being [t, s, a, r, s']
#transition data must be generated by the EVALUATION policy.
def on_policy_Q_estimate(transition_data, discount_factor):
    #Separate relevant parts of the data
    t_list = transition_data[..., 0].astype(int)
    r_list = transition_data[..., 3].astype(float)

    # Per-trajectory returns (discounted cumulative rewards)
    G = (r_list * np.power(discount_factor, t_list)).sum(axis=-1)
    return np.average(G)

#=============================TRADITIONAL ESTIMATORS=============================================================

#transition_data is a NumPy tensor of shape (N, T, 5) with the last last dimension being [t, s, a, r, s']
#transition data must be generated by the BEHAVIOUR policy.
def off_policy_IS_estimator(transition_data, discount_factor, pi_b, pi_e):
    #Separate relevant parts of the data
    t_list = transition_data[..., 0].astype(int)
    s_list = transition_data[..., 1].astype(int)
    a_list = transition_data[..., 2].astype(int)
    r_list = transition_data[..., 3].astype(float)

    # Per-trajectory returns (discounted cumulative rewards)
    G = (r_list * np.power(discount_factor, t_list)).sum(axis=-1)

    # Per-transition importance ratios
    p_b = pi_b[s_list, a_list]
    p_e = pi_e[s_list, a_list]
    # Per-trajectory cumulative importance ratios, take the product
    rho = (p_e / p_b).prod(axis=-1)
    #Weight with IS ratio then average
    return np.average(G*rho)

#transition_data is a NumPy tensor of shape (N, T, 5) with the last last dimension being [t, s, a, r, s']
#transition data must be generated by the BEHAVIOUR policy.
def off_policy_PDIS_estimator(transition_data, discount_factor, pi_b, pi_e):
    #Separate relevant parts of the data
    t_list = transition_data[..., 0].astype(int)
    s_list = transition_data[..., 1].astype(int)
    a_list = transition_data[..., 2].astype(int)
    r_list = transition_data[..., 3].astype(float)

    #Transition probabilities
    p_b = pi_b[s_list, a_list]
    p_e = pi_e[s_list, a_list]
    # Per-decision cumulative importance ratios, take the product
    rhos = (p_e / p_b).cumprod(axis=-1)

    # Per-trajectory returns (discounted cumulative rewards)
    G = (rhos * r_list * np.power(discount_factor, t_list)).sum(axis=-1)

    return np.average(G)

#transition_data is a NumPy tensor of shape (N, T, 5) with the last last dimension being [t, s, a, r, s']
#transition data must be generated by the BEHAVIOUR policy.
def off_policy_PDWIS_estimator(transition_data, discount_factor, pi_b, pi_e):
    #Separate relevant parts of the data
    t_list = transition_data[..., 0].astype(int)
    s_list = transition_data[..., 1].astype(int)
    a_list = transition_data[..., 2].astype(int)
    r_list = transition_data[..., 3].astype(float)

    #Transition probabilities
    p_b = pi_b[s_list, a_list]
    p_e = pi_e[s_list, a_list]
    # Per-decision cumulative importance ratios, take the product
    rhos = (p_e / p_b).cumprod(axis=-1)

    # Per-trajectory returns (discounted cumulative rewards)
    G = (rhos * r_list * np.power(discount_factor, t_list)).sum(axis=-1)

    return np.sum(G)/np.sum(rhos)


#=============================FACTORED ESTIMATORS=============================================================

#transition_data is a NumPy tensor of shape (N, T, D, 5) with the last last dimension being [t, z, a, r, z']
#transition data must be generated by the BEHAVIOUR policy.
def off_policy_DecIS_estimator(transition_data,
                               discount_factor,
                               action_spaces,
                               factored_behaviour_policies,
                               factored_evaluation_policies):
    decomposed_Qs = np.zeros(len(action_spaces))
    for d in range(len(action_spaces)):
        #Separate relevant parts of the data
        t_list = transition_data[..., d, 0].astype(int)
        s_list = transition_data[..., d, 1].astype(int)
        a_list = transition_data[..., d, 2].astype(int)
        r_list = transition_data[..., d, 3].astype(float)

        # Per-trajectory returns (discounted cumulative rewards)
        G = (r_list * np.power(discount_factor, t_list)).sum(axis=-1)
        
        #Transition probabilities
        p_b = factored_behaviour_policies[d][s_list, a_list]
        p_e = factored_evaluation_policies[d][s_list, a_list]

        # Per-trajectory cumulative importance ratios, take the product
        rho = (p_e / p_b).prod(-1)
        #Weight with IS ratio then average
        decomposed_Qs[d] = np.average(G*rho)
        
    return np.sum(decomposed_Qs), decomposed_Qs



#transition_data is a NumPy tensor of shape (N, T, D, 5) with the last last dimension being [t, z, a, r, z']
#transition data must be generated by the BEHAVIOUR policy.
def off_policy_DecPDIS_estimator(transition_data,
                               discount_factor,
                               action_spaces,
                               factored_behaviour_policies,
                               factored_evaluation_policies):
    decomposed_Qs = np.zeros(len(action_spaces))
    for d in range(len(action_spaces)):
        #Separate relevant parts of the data
        t_list = transition_data[..., d, 0].astype(int)
        s_list = transition_data[..., d, 1].astype(int)
        a_list = transition_data[..., d, 2].astype(int)
        r_list = transition_data[..., d, 3].astype(float)

        #Transition probabilities
        p_b = factored_behaviour_policies[d][s_list, a_list]
        p_e = factored_evaluation_policies[d][s_list, a_list]
        # Per-decision cumulative importance ratios, take the product
        rhos = (p_e / p_b).cumprod(axis=-1)

        # Per-trajectory returns (discounted cumulative rewards)
        G = (rhos * r_list * np.power(discount_factor, t_list)).sum(axis=-1)

        decomposed_Qs[d] = np.average(G)

    return np.sum(decomposed_Qs), decomposed_Qs



#transition_data is a NumPy tensor of shape (N, T, D, 5) with the last last dimension being [t, z, a, r, z']
#transition data must be generated by the BEHAVIOUR policy.
def off_policy_DecPDWIS_estimator(transition_data,
                               discount_factor,
                               action_spaces,
                               factored_behaviour_policies,
                               factored_evaluation_policies):
    decomposed_Qs = np.zeros(len(action_spaces))
    for d in range(len(action_spaces)):
        #Separate relevant parts of the data
        t_list = transition_data[..., d, 0].astype(int)
        s_list = transition_data[..., d, 1].astype(int)
        a_list = transition_data[..., d, 2].astype(int)
        r_list = transition_data[..., d, 3].astype(float)

        #Transition probabilities
        p_b = factored_behaviour_policies[d][s_list, a_list]
        p_e = factored_evaluation_policies[d][s_list, a_list]
        # Per-decision cumulative importance ratios, take the product
        rhos = (p_e / p_b).cumprod(axis=-1)

        # Per-trajectory returns (discounted cumulative rewards)
        G = (rhos * r_list * np.power(discount_factor, t_list)).sum(axis=-1)

        decomposed_Qs[d] = np.sum(G)/np.sum(rhos)
        
    return np.sum(decomposed_Qs), decomposed_Qs
